import pandas as pd
import pyarrow.parquet as pq
import gcsfs
from google.cloud import bigquery
# import os
# import glob

PROJECT_ID = "turing-thought-481409-d8"
DATASET_ID = "kltn"
TABLE_ID = "traindatareviewtest"

TRAIN_PATH = "gs://kltn--data/train_df20/*.parquet"
TEST_PATH = "gs://kltn--data/val_df20/*.parquet"

def load_parquet_from_gcs(path_pattern):
    """ƒê·ªçc t·∫•t c·∫£ file Parquet matching pattern t·ª´ GCS"""
    fs = gcsfs.GCSFileSystem()
    files = fs.glob(path_pattern)
    tables = [pq.read_table(fs.open(f, "rb")) for f in files]
    return pd.concat([t.to_pandas() for t in tables], ignore_index=True)

# def load_parquet_from_folder(folder_path):            
#     """ƒê·ªçc t·∫•t c·∫£ file Parquet trong m·ªôt folder local"""
#     folder_path = os.path.abspath(folder_path)
#     files = glob.glob(os.path.join(folder_path, "*.parquet"))
#     tables = [pq.read_table(f) for f in files]
#     return pd.concat([t.to_pandas() for t in tables], ignore_index=True)

def parse_dt(df, cols=["timestamp"]):
    return df.assign(
        **{
            col: lambda df: pd.to_datetime(df[col].astype(int), unit="ms", utc=True)
            for col in cols
        }
    )

def concat_and_upload():
    print("üì• ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ GCS...")
    df_train = load_parquet_from_gcs(TRAIN_PATH)
    df_test = load_parquet_from_gcs(TEST_PATH)

    print(f"Train: {len(df_train)} d√≤ng | Test: {len(df_test)} d√≤ng")

    # G·ªôp 2 t·∫≠p
    df_all = pd.concat([df_train, df_test], ignore_index=True).pipe(parse_dt)

    # X√≥a c·ªôt images n·∫øu t·ªìn t·∫°i
    if "images" in df_all.columns:
        df_all = df_all.drop(columns=["images"])
        print("üóëÔ∏è ƒê√£ x√≥a c·ªôt 'images' tr∆∞·ªõc khi upload")

    print(f"T·ªïng c·ªông: {len(df_all)} d√≤ng sau khi concat")

    # Upload l√™n BigQuery
    client = bigquery.Client(project=PROJECT_ID)
    table_ref = client.dataset(DATASET_ID).table(TABLE_ID)

    job_config = bigquery.LoadJobConfig(
        write_disposition="WRITE_TRUNCATE",  
        autodetect=True
    )

    job = client.load_table_from_dataframe(df_all, table_ref, job_config=job_config)
    job.result()  # Ch·ªù job ho√†n t·∫•t

    print(f"ƒê√£ upload {len(df_all)} d√≤ng l√™n b·∫£ng {DATASET_ID}.{TABLE_ID}")

if __name__ == "__main__":
    concat_and_upload()
