apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: ranker-model-server
  namespace: serving
  annotations:
    autoscaling.knative.dev/minScale: "0"
    autoscaling.knative.dev/maxScale: "3"
spec:
  predictor:
    containers:
      - name: predictor
        image: quangtran1011/ranker_model_server:kserve7
        command: ["bentoml", "serve", "service:RankerService", "--port", "3000"]
        ports:
          - containerPort: 3000
        env:
          - name: MLFLOW_TRACKING_URI
            value: "http://34.69.242.168:8080"

          - name: QDRANT_HOST
            value: "http://qdrant"
          - name: QDRANT_PORT
            value: "6333"
          - name: MODEL_VERSION
            value: "0"

        readinessProbe:
          exec:
            command:
            - curl
            - -X
            - POST
            - -f
            - http://localhost:3000/ready_check
          initialDelaySeconds: 150
          periodSeconds: 30
          failureThreshold: 20
